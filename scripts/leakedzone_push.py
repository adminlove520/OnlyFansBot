import asyncio
import logging
import os
import sys
import json
import argparse
from datetime import datetime
import random
import httpx
from dotenv import load_dotenv

VERSION = "1.0.0"
try:
    version_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "VERSION")
    if os.path.exists(version_path):
        with open(version_path, "r") as f:
            VERSION = f.read().strip()
except:
    pass

REPO_URL = "https://github.com/adminlove520/OnlyFansBot" # ä»“åº“åœ°å€

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ crawlers æ¨¡å—
# æ— è®ºè„šæœ¬ä»å“ªé‡Œè¿è¡Œï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ° crawlers åŒ…
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from crawlers.leakedzone import LeakedZoneCrawler

# åŠ è½½é…ç½®
load_dotenv(override=True)

# æ—¥å¿—é…ç½® (å¿…é¡»åœ¨ load_dotenv ä¹‹åå®šä¹‰ logger æ¶‰åŠçš„ context)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("LZ-Push-v6")

DB_FILE = "data/lz_history.json"
# å‰¥ç¦»å•åŒå¼•å·
WEBHOOK_URL = os.getenv("LZ_WEBHOOK_URL", "").strip("'").strip('"')
if not WEBHOOK_URL:
    logger.error("âŒ LZ_WEBHOOK_URL ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ .env é…ç½®")

def load_history():
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, "r") as f: return set(json.load(f))
        except: return set()
    return set()

def save_history(history):
    os.makedirs("data", exist_ok=True)
    with open(DB_FILE, "w") as f: json.dump(list(history), f)

async def send_startup_card(platforms):
    """å‘é€å¯åŠ¨é€šçŸ¥å¡ç‰‡"""
    logger.info("ğŸ“¡ å‘é€å¯åŠ¨é€šçŸ¥å¡ç‰‡...")
    start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    embed = {
        "title": "ğŸ›°ï¸ LeakedZone æ¶©æ¶©å…ˆé”‹",
        "url": REPO_URL,
        "description": f"æ¬¢è¿ä½¿ç”¨ **OnlyFans-Bot** è‡ªåŠ¨åŒ–æƒ…æŠ¥æœåŠ¡ã€‚\næ‰«æç¨‹åºå·²å°±ç»ªï¼Œæ­£åœ¨ç²¾å‡†æ•è·æœ€æ–°æ¶©æ¶©ã€‚",
        "color": 0x00ff00,
        "fields": [
            {"name": "å½“å‰ç‰ˆæœ¬", "value": f"`{VERSION}`", "inline": True},
            {"name": "å¯åŠ¨æ—¶é—´", "value": f"`{start_time}`", "inline": True},
            {"name": "ç›‘æ§èŒƒå›´", "value": f"å…± `{len(platforms)}` ä¸ªå¹³å° ", "inline": False},
            {"name": "å¼€æºä»“åº“", "value": f"[OnlyFans-Bot @ GitHub]({REPO_URL})", "inline": False}
        ],
        "footer": {
            "text": f"æƒ…æŠ¥åŒæ­¥ä¸­ | Power By OnlyFans-Bot"
        }
    }
    await send_webhook(embed)

async def fetch_douban_movies():
    """è·å–è±†ç“£æ–°ç‰‡æ¦œ"""
    url = "https://api.baiwumm.com/api/douban-movic" 
    try:
        # trust_env=False å¼ºåˆ¶ä¸ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼Œverify=False å¿½ç•¥ SSL è¯ä¹¦é”™è¯¯(æé«˜å…¼å®¹æ€§)
        async with httpx.AsyncClient(trust_env=False, verify=False) as client:
            res = await client.get(url, timeout=15)
            if res.status_code == 200:
                data = res.json()
                if "data" in data: return data["data"]
    except Exception as e:
        logger.warning(f"è·å–è±†ç“£ç”µå½±å¤±è´¥ (è¯·æ£€æŸ¥ç½‘ç»œæˆ– API): {e}")
    return []

async def push_movie_item(movie):
    """æ¨é€å•æ¡ç”µå½±ä¿¡æ¯ """
    title = movie.get("title", "æœªçŸ¥ç”µå½±")
    score = movie.get("score") or "N/A"
    hot = movie.get("hot", 0)
    douban_url = movie.get("url", "https://movie.douban.com")
    
    desc_lines = []
    desc_lines.append(f"**è¯„åˆ†**: â­ `{score}`")
    if hot > 0:
        desc_lines.append(f"**çƒ­åº¦**: ğŸ”¥ `{hot}`")
    
    embed = {
        "title": f"ğŸ¿ è±†ç“£æ–°ç‰‡ï¼š{title}",
        "url": douban_url,
        "description": "\n".join(desc_lines),
        "color": 0x00BB29, # è±†ç“£ç»¿
        "fields": [
            {"name": "ç›´è¾¾é€šé“", "value": f"[ğŸ”— ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…(è±†ç“£)]({douban_url})", "inline": True}
        ],
        "footer": {"text": f"OnlyFans-Bot è±†ç“£ç²¾é€‰Â·æ–°ç‰‡é€Ÿé€’"}
    }
    await send_webhook(embed)

async def send_webhook(embed):
    if not WEBHOOK_URL:
        logger.error("æœªé…ç½® LZ_WEBHOOK_URL")
        return
    proxy_url = os.getenv("HTTP_PROXY")
    async with httpx.AsyncClient(proxy=proxy_url) as client:
        for i in range(3):
            try:
                res = await client.post(WEBHOOK_URL, json={"embeds": [embed]}, timeout=15)
                if res.status_code in [200, 204]:
                    return
                # å¦‚æœæ˜¯ 429 é™æµï¼Œå¤šç­‰ä¸€ä¼š
                if res.status_code == 429:
                    await asyncio.sleep(5)
                logger.warning(f"Webhook å‘é€å¤±è´¥ (å°è¯• {i+1}/3): {res.status_code}")
                await asyncio.sleep(2)
            except Exception as e:
                logger.warning(f"Webhook ç½‘ç»œå¼‚å¸¸ (å°è¯• {i+1}/3): {e}")
                await asyncio.sleep(2)
        logger.error("âŒ Webhook å‘é€æœ€ç»ˆå¤±è´¥")

async def push_item(crawler: LeakedZoneCrawler, item):
    """æ„å»ºå¡ç‰‡ (æ¶©æ¶©é€‚åº¦) é’ˆå¯¹ç”¨æˆ·é“¾æ¥è·³è½¬æ·±åº¦ä¼˜åŒ–"""
    tag = item['tag']
    username = item['username']
    post_id = item['post_id']
    is_video = item['is_video']
    
    # è·å–åˆ›ä½œè€…æ‰€å±å¹³å° (Category)
    platform = await crawler.get_creator_platform(username)
    
    # 1. é“¾æ¥é€»è¾‘ä¼˜åŒ–
    profile_url = f"https://leakedzone.com/{username}"
    # æ ‡é¢˜æŒ‡å‘å…·ä½“ç±»å‹é¡µ: /username/photo æˆ– /username/video
    type_suffix = "video" if is_video else "photo"
    type_url = f"{profile_url}/{type_suffix}"
    # å¹³å°åˆ—è¡¨æŒ‡å‘åˆ†ç±»é¡µ: /creators?Category=Reddit
    category_list_url = f"https://leakedzone.com/creators?Category={platform}"
    
    title_type = "Videos" if is_video else "Photos"
    title = f"LeakedZone-{title_type}åŠ¨æ€"
    color = random.randint(0, 0xFFFFFF)
    
    embed = {
        "title": title,
        "url": type_url,
        "description": f"å‘ç°æ¥è‡ªåˆ›ä½œè€… **[@{username}]({profile_url})** çš„æ–°åŠ¨æ€ã€‚\n\n> ğŸ†” å”¯ä¸€æ ‡è¯†: `{post_id}`\n> ğŸ·ï¸ æºæ ‡ç­¾: `{tag}`",
        "color": color,
        "fields": [
            {
                "name": "åˆ›ä½œè€…",
                "value": f"[@{username}]({profile_url})",
                "inline": True
            },
            {
                "name": "åˆ†ç±»",
                "value": f"Category: [{platform}]({category_list_url}) | {'è§†é¢‘' if is_video else 'å›¾ç‰‡'}",
                "inline": True
            }
        ],
        "footer": {
            "text": f"OnlyFans-Bot æƒ…æŠ¥å…ˆé”‹ â€¢ {datetime.now().strftime('%H:%M')}"
        }
    }
    
    # é¢„è§ˆå›¾é€»è¾‘
    if item['img_url'] and "default" not in item['img_url']:
        embed["image"] = {"url": item['img_url']}
        embed["thumbnail"] = {"url": item['img_url']}
    
    # å¢åŠ ç‚¹å‡»è¯¦æƒ… (æŒ‡å‘å…·ä½“åŠ¨æ€å¸–)
    embed["fields"].append({
        "name": "è®¿é—®è¯¦æƒ…",
        "value": f"[ğŸ”— ç‚¹å‡»è®¿é—®è¯¦æƒ…]({item['url']})",
        "inline": False
    })

    if item['created_at']:
        embed["fields"].append({
            "name": "å‘å¸ƒæ—¶é—´",
            "value": f"`{item['created_at']}`",
            "inline": True
        })

    await send_webhook(embed)

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--login", help="Update cookies and UA")
    args = parser.parse_args()

    crawler = LeakedZoneCrawler()
    
    if args.login:
        parts = args.login.split('|')
        cookie = parts[0]
        ua = parts[1] if len(parts) > 1 else None
        crawler.set_auth(cookie, ua)
        logger.info("âœ… è®¤è¯å‡­æ®å·²æ›´æ–°")
        if await crawler.check_auth():
            logger.info("âœ¨ å‡­æ®æ ¡éªŒæˆåŠŸï¼")
        else:
            logger.error("âŒ å‡­æ®æ ¡éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥")
        return

    # --- 1. åˆå§‹åŒ–å˜é‡ä¸ç¯å¢ƒ ---
    history = load_history()
    all_items = []
    
    # åˆå§‹åŒ–ç›‘æ§å¹³å°åˆ—è¡¨
    platforms = ["OnlyFans", "Fansly", "Celebrity+Nudes", "Reddit", "Snapchat"]
    try:
        # ä¼˜å…ˆè¯»å–é…ç½®æ–‡ä»¶
        cat_file = "crawlers/leakedzone-category.json"
        if not os.path.exists(cat_file): cat_file = "data/lz_auth.json"
        
        with open(cat_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            if "categories" in data and isinstance(data["categories"], list):
                platforms = data["categories"]
                logger.info(f"å·²åŠ è½½ç›‘æ§å¹³å°: {platforms}")
    except: pass

    # --- 2. æƒé™æ ¡éªŒ ---
    if not await crawler.check_auth():
        logger.error("ğŸš¨ æ— æ³•é€šè¿‡ LeakedZone éªŒè¯ï¼ˆCloudflare æ‹¦æˆªæˆ– Cookie è¿‡æœŸï¼‰")
        logger.info("ğŸ’¡ æ­£åœ¨å°è¯•è‡ªåŠ¨è¿è¡Œåˆ·æ–°è„šæœ¬...")
        try:
            refresh_script = os.path.join(project_root, "scripts", "lz_refresh.py")
            process = await asyncio.create_subprocess_exec(
                sys.executable, refresh_script,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            if process.returncode == 0:
                logger.info("âœ… è‡ªåŠ¨åŒ–åˆ·æ–°è„šæœ¬æ‰§è¡ŒæˆåŠŸï¼Œæ­£åœ¨é‡æ–°åˆå§‹åŒ–...")
                crawler = LeakedZoneCrawler() # é‡æ–°åŠ è½½å‡­æ®
                if await crawler.check_auth():
                    logger.info("âœ¨ åˆ·æ–°åæ ¡éªŒæˆåŠŸï¼")
                else:
                    logger.error("âŒ åˆ·æ–°åä¾ç„¶æ ¡éªŒå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ scripts/lz_refresh.py")
                    return
            else:
                logger.error(f"âŒ è‡ªåŠ¨åŒ–åˆ·æ–°è„šæœ¬æ‰§è¡Œå¤±è´¥: {stderr.decode()}")
                return
        except Exception as e:
            logger.error(f"âŒ è§¦å‘åˆ·æ–°è„šæœ¬å¼‚å¸¸: {e}")
            return

    # --- 3. å‘é€å¯åŠ¨é€šæŠ¥ ---
    await send_startup_card(platforms)
    
    # --- 4. æ‰§è¡Œé‡‡é›† ---
    logger.info("âœ…æ­£åœ¨é‡‡é›† (å½“æ—¥è§†é¢‘&å›¾ç‰‡)...")
    all_items.extend(await crawler.crawl_tag("videos"))
    all_items.extend(await crawler.crawl_tag("photos"))

    logger.info("âœ…æ­£åœ¨é‡‡é›†å„å¹³å°è¯¦æƒ…åŠ¨æ€...")
    for p in platforms:
        all_items.extend(await crawler.crawl_category(p))
        await asyncio.sleep(1)

    # 3. å»é‡ä¸æ¨é€
    new_count = 0
    try:
        for item in all_items:
            unique_key = f"{item['tag']}_{item['post_id']}"
            if unique_key not in history:
                logger.info(f"ğŸ†• å‘ç°æ–°åŠ¨æ€: @{item['username']} ({item['tag']})")
                await push_item(crawler, item)
                history.add(unique_key)
                new_count += 1
                # å¢é‡ä¿å­˜ï¼Œé˜²æ­¢ä¸­é€”ç”±äºç½‘ç»œå¼‚å¸¸æˆ–é™æµå¯¼è‡´è®°å½•ä¸¢å¤±
                save_history(history)
                await asyncio.sleep(1)
            
            if new_count >= 30: break # å•æ¬¡ä¸Šé™
    except Exception as e:
        logger.error(f"âš ï¸ æ¨é€è¿‡ç¨‹ä¸­æ–­: {e}")
    finally:
        save_history(history)
        logger.info(f"âœ… å¤„ç†å®Œæ¯•ï¼Œå½“å‰å‘¨æœŸæ–°å¢: {new_count}")

    # 4. æ¨é€è±†ç“£æ–°ç‰‡ (ä½œä¸ºç¦åˆ©ç¯èŠ‚)
    logger.info("ğŸ¿ ï¼ˆæˆ’è‰²ï¼Œæ¥ç‚¹å°æ¸…æ–°~ï¼‰æ­£åœ¨è·å–è±†ç“£æ–°ç‰‡æ¦œ...")
    movies = await fetch_douban_movies()
    if movies:
        # å‘é€è½¬åœºåˆ†éš”å¡ç‰‡
        await send_webhook({
            "title": "âœ… æƒ…æŠ¥æ‰«æä»»åŠ¡åœ†æ»¡å®Œæˆ",
            "description": "æ‰€æœ‰å½“æ—¥åŠ¨æ€å·²å¤„ç†å®Œæ¯•ï¼Œ **OnlyFans-Bot** è±†ç“£ç²¾é€‰ï¼š\nğŸ¿ **ä»Šæ—¥è±†ç“£æ–°ç‰‡é€Ÿé€’**",
            "color": 0x00BB29
        })
        await asyncio.sleep(2)

        logger.info(f"ğŸ“Š å‘ç° {len(movies)} éƒ¨æ–°ç‰‡ï¼Œæ­£åœ¨æ¨é€...")
        for m in movies:
            await push_movie_item(m)
            await asyncio.sleep(1)
    
    logger.info(f"âœ¨ è¿™ä¸€è½®æ¨é€å·¥ä½œå·²åœ†æ»¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
