import logging
import asyncio
import os
import json
import time
from datetime import datetime
from curl_cffi.requests import AsyncSession
from bs4 import BeautifulSoup as bs
from crawlers.base import BaseCrawler
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)

class LeakedZoneCrawler(BaseCrawler):
    def __init__(self):
        super().__init__(base_url="https://leakedzone.com")
        self.source_name = "LeakedZone"
        self.platform = "leakedzone"
        self._session = None
        self.auth_file = "data/lz_auth.json"
        self.platform_cache_file = "data/lz_platforms.json"
        
        # é»˜è®¤é…ç½®
        self.cookie_str = ""
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
        self.platforms_cache = {}
        
        self._load_auth()
        self._load_platforms()

    def _load_auth(self):
        """åŠ è½½æŒä¹…åŒ–å‡­æ®ï¼šlz_auth.json æ˜¯å”¯ä¸€çœŸç†æ¥æº"""
        if os.path.exists(self.auth_file):
            try:
                with open(self.auth_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.cookie_str = data.get("cookie", "")
                    self.user_agent = data.get("ua", self.user_agent)
                    updated_at = data.get("updated_at", "æœªçŸ¥")
                    logger.info(f"å·²åŠ è½½æŒä¹…åŒ–å‡­æ® (æ›´æ–°æ—¶é—´: {updated_at})")
            except Exception as e:
                logger.warning(f"åŠ è½½ auth_file å¤±è´¥: {e}")
        
        # å…¼å®¹æ€§æ£€æŸ¥ï¼šå¦‚æœæ˜¯ç¯å¢ƒå˜é‡
        ec = os.getenv("LEAKEDZONE_COOKIES", "").strip("'").strip('"')
        if ec and not self.cookie_str:
            logger.info("æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ä¸­çš„æ—§ Cookieï¼Œæ­£åœ¨è¿ç§»...")
            self.cookie_str = ec
            self.set_auth(ec)

    def set_auth(self, cookie: str, ua: str = None):
        """æ›´æ–°å¹¶ä¿å­˜å‡­æ®"""
        self.cookie_str = cookie
        if ua: self.user_agent = ua
        os.makedirs("data", exist_ok=True)
        data = {
            "cookie": self.cookie_str, 
            "ua": self.user_agent,
            "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.auth_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4)
        self._session = None # å¼ºåˆ¶åˆ·æ–° session
        logger.info("âœ… æˆæƒå‡­æ®å·²æ›´æ–°å¹¶ä¿å­˜")

    def _load_platforms(self):
        """åŠ è½½å·²çŸ¥çš„åˆ›ä½œè€…å¹³å°ç¼“å­˜"""
        if os.path.exists(self.platform_cache_file):
            try:
                with open(self.platform_cache_file, "r", encoding="utf-8") as f:
                    self.platforms_cache = json.load(f)
            except: pass

    def _save_platforms(self):
        """ä¿å­˜åˆ›ä½œè€…å¹³å°ç¼“å­˜"""
        os.makedirs("data", exist_ok=True)
        with open(self.platform_cache_file, "w", encoding="utf-8") as f:
            json.dump(self.platforms_cache, f, indent=4)

    async def get_creator_platform(self, username: str) -> str:
        """è·å–åˆ›ä½œè€…å¯¹åº”çš„å¹³å° (å¦‚ Reddit), å…·æœ‰ç¼“å­˜æœºåˆ¶"""
        if username in self.platforms_cache:
            return self.platforms_cache[username]
        
        # å¦‚æœç¼“å­˜æ²¡æœ‰ï¼Œåˆ™å»çˆ¬ä¸€ä¸‹ profile é¡µ
        url = f"{self.base_url}/{username}"
        logger.info(f"ğŸ” æ­£åœ¨æŠ“å–åˆ›ä½œè€…ä¸»é¡µä»¥è¯†åˆ«åˆ†ç±»: {url}")
        html = await self.fetch_html(url)
        if not html: return "Unknown"
        
        try:
            soup = bs(html, 'lxml')
            # æ–¹æ¡ˆ A: å¯»æ‰¾åŒ…å« Category æ–‡å­—çš„å…ƒç´ 
            # å¸¸è§ç»“æ„: <b>Category:</b> <a href="...">OnlyFans</a>
            platform = "Unknown"
            
            # ä½¿ç”¨ç²¾å‡†æŸ¥æ‰¾
            cat_container = soup.find(string=lambda s: s and "Category:" in s)
            if cat_container:
                # é€šå¸¸æ˜¯ <b>Category:</b> åé¢çš„ <a>
                parent = cat_container.parent
                cat_val = parent.find_next_sibling('a') or parent.find('a')
                if not cat_val:
                    # å°è¯•åœ¨çˆ¶çº§çš„å…„å¼Ÿä¸­æ‰¾
                    cat_val = parent.find_next('a')
                
                if cat_val:
                    platform = cat_val.text.strip()
                    logger.info(f"âœ¨ è¯†åˆ«åˆ°åˆ›ä½œè€… {username} å¹³å°: {platform}")
            
            # æ–¹æ¡ˆ B: å¦‚æœ A å¤±è´¥ï¼Œå°è¯• model-info åŒºåŸŸ
            if platform == "Unknown":
                model_info = soup.find('div', class_='model-info')
                if model_info:
                    cat_link = model_info.find('a', href=lambda h: h and 'Category=' in h)
                    if cat_link: platform = cat_link.text.strip()

            if platform != "Unknown":
                self.platforms_cache[username] = platform
                self._save_platforms()
                return platform
        except Exception as e:
            logger.warning(f"è§£æåˆ›ä½œè€… {username} å¹³å°æ—¶å‡ºé”™: {e}")
        
        return "Unknown"

    async def _get_session(self) -> AsyncSession:
        if not self._session:
            # ä»£ç†é…ç½®
            proxy = os.getenv("HTTP_PROXY")
            
            # åŸºç¡€ Header æ„é€ 
            headers = {
                "User-Agent": self.user_agent,
                "Referer": "https://leakedzone.com/",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Upgrade-Insecure-Requests": "1"
            }

            # ã€GHA ä¸“é¡¹ä¼˜åŒ–ã€‘åœ¨ Linux ç¯å¢ƒä¸‹ï¼Œimpersonate å»ºè®®é€‰æ‹©æ›´ä½æˆ–æ›´ç¨³çš„ç‰ˆæœ¬
            # å¹¶ä¸”éœ€è¦è¡¥å…¨å®¢æˆ·ç«¯æŒ‡çº¹å¤´
            imp = "chrome120"
            if "Linux" in self.user_agent:
                imp = "chrome110"
                headers["Sec-Ch-Ua-Platform"] = '"Linux"'
            else:
                headers["Sec-Ch-Ua-Platform"] = '"Windows"'
            
            # ä½¿ç”¨ curl_cffi æ¨¡æ‹Ÿæµè§ˆå™¨ç‰¹å¾
            self._session = AsyncSession(
                impersonate=imp,
                headers=headers,
                proxies={"http": proxy, "https": proxy} if proxy else None,
                allow_redirects=True,
                timeout=40
            )
            
            # æ‰‹åŠ¨åˆå¹¶ Cookie
            if self.cookie_str:
                for item in self.cookie_str.split(';'):
                    item = item.strip()
                    if '=' in item:
                        k, v = item.split('=', 1)
                        if k.strip() and v.strip():
                            self._session.cookies.set(k.strip(), v.strip(), domain="leakedzone.com")
            
            if proxy: logger.info(f"å·²å¯ç”¨ä»£ç† (curl_cffi): {proxy}")
        return self._session

    async def check_auth(self) -> bool:
        """å¼ºåŒ–ç‰ˆè¿é€šæ€§ä¸ CF æŒ‘æˆ˜æ£€æŸ¥"""
        try:
            session = await self._get_session()
            res = await session.get(f"{self.base_url}/videos")
            
            if res.status_code == 200:
                # æ£€æŸ¥ä¼ªè£…åçš„ Cloudflare é¡µé¢
                if "Just a moment" in res.text or "Checking your browser" in res.text:
                    logger.error("ğŸ›‘ ä¾ç„¶è¢« Cloudflare æ‹¦æˆª (å³ä½¿çŠ¶æ€ç ä¸º 200)")
                    return False
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†…å®¹
                if "movie-item" in res.text or "videos" in res.url:
                    return True
                
            if res.status_code == 403:
                logger.error("ğŸš« æŠ“å–è¢«æ‹’ (403): Cookie å¯èƒ½å·²è¿‡æœŸ")
            else:
                logger.error(f"Auth check failed: Status {res.status_code}")
            
            return False
        except Exception as e:
            logger.error(f"Auth check error: {e}")
            return False

    async def fetch_html(self, url: str) -> Optional[str]:
        """ç»Ÿä¸€ä¸‹è½½æ¥å£"""
        session = await self._get_session()
        try:
            res = await session.get(url)
            if res.status_code == 200:
                if "Just a moment" in res.text:
                    logger.warning("å‘ç° Cloudflare æŒ‘æˆ˜ï¼Œå¯èƒ½éœ€è¦åˆ·æ–° Cookie")
                    return None
                return res.text
            logger.warning(f"è¯·æ±‚ {url} å¤±è´¥: {res.status_code}")
        except Exception as e:
            logger.error(f"LZ è¯·æ±‚å¼‚å¸¸: {e}")
        return None

    def _parse_items(self, html: str, tag: str) -> List[Dict[str, Any]]:
        """æç®€è§£æå™¨ï¼šä»…æå–æ ¸å¿ƒå…ƒæ•°æ®"""
        soup = bs(html, 'lxml')
        
        items = soup.find_all(['div', 'article'], class_=['movie-item', 'light-gallery-item', 'model-item'])
        posts = []
        
        today_str = datetime.now().strftime('%Y.%m.%d')

        for item in items:
            try:
                # æå–é“¾æ¥ä¸ ID
                link_elem = item.find('a')
                if not link_elem: continue
                
                raw_href = link_elem.get('href', '').strip()
                href = raw_href.replace('https://leakedzone.com/', '').strip('/')
                
                parts = href.split('/')
                if not parts: continue
                
                # åˆ›ä½œè€…é¡µé¢é€šå¸¸æ˜¯ /onlyfans-creators æˆ– /username
                item_class = item.get('class', [])
                
                date_elem = item.find('span', class_='date')
                created_at = date_elem.text.strip() if date_elem else None
                
                # æ—¥æœŸè¿‡æ»¤ï¼šä»…é’ˆå¯¹ Videos/Photos
                if tag in ['Videos', 'Photos']:
                    if not created_at or created_at != today_str: continue

                # ç”¨æˆ·åæå–
                username = "unknown"
                # åˆ¤å®šæ˜¯å¦ä¸ºåˆ›ä½œè€…å¡ç‰‡: 
                # 1. æ˜¾å¼åŒ…å« model-item ç±»
                # 2. è·¯å¾„ä»…æœ‰ä¸€æ®µ (ä¾‹å¦‚ /onlyfans) ä¸”ä¸åœ¨ Videos/Photos æ ‡ç­¾ä¸‹
                is_creator_card = 'model-item' in item_class or (tag not in ['Videos', 'Photos'] and len(parts) == 1)
                
                if is_creator_card: 
                    username = parts[0]
                    post_id = f"model_{username}"
                else: # è§†é¢‘/åŠ¨æ€é¡¹
                    if len(parts) >= 2: username = parts[0]
                    post_id = parts[-1]

                if username == "https:": continue
                
                # æå–é¢„è§ˆå›¾
                img_elem = item.find('img')
                img_url = img_elem.get('src') if img_elem else ""
                if img_url.startswith('//'): img_url = f"https:{img_url}"
                elif img_url.startswith('/'): img_url = f"{self.base_url}{img_url}"
                elif img_url.startswith('data:image'): 
                    # è¿‡æ»¤ Base64 å›¾ç‰‡ï¼Œé¿å… Webhook 400 Failed
                    img_url = ""

                posts.append({
                    "post_id": post_id,
                    "username": username,
                    "url": f"{self.base_url}/{href}",
                    "img_url": img_url,
                    "created_at": created_at,
                    "tag": tag,
                    "is_video": "video" in href or item.find('span', class_='play-icon') is not None
                })
            except: continue
            
        return posts

    async def crawl_tag(self, tag_path: str) -> List[Dict[str, Any]]:
        """æŠ“å– /videos æˆ– /photos"""
        html = await self.fetch_html(f"{self.base_url}/{tag_path}")
        tag_name = "Videos" if tag_path == "videos" else "Photos"
        return self._parse_items(html, tag_name) if html else []

    async def crawl_category(self, cat_name: str) -> List[Dict[str, Any]]:
        """æŠ“å– /creators?Category=OnlyFans ç­‰"""
        html = await self.fetch_html(f"{self.base_url}/creators?Category={cat_name}")
        return self._parse_items(html, cat_name) if html else []
