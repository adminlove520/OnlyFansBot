import asyncio
import logging
import os
import sys
import json
import argparse
from datetime import datetime
import random
import httpx
from dotenv import load_dotenv

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ crawlers æ¨¡å—
# æ— è®ºè„šæœ¬ä»å“ªé‡Œè¿è¡Œï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ° crawlers åŒ…
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from crawlers.leakedzone import LeakedZoneCrawler

# åŠ è½½é…ç½®
load_dotenv(override=True)

# æ—¥å¿—é…ç½® (å¿…é¡»åœ¨ load_dotenv ä¹‹åå®šä¹‰ logger æ¶‰åŠçš„ context)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("LZ-Push-v6")

DB_FILE = "data/lz_history.json"
# å‰¥ç¦»å•åŒå¼•å·
WEBHOOK_URL = os.getenv("LZ_WEBHOOK_URL", "").strip("'").strip('"')
if not WEBHOOK_URL:
    logger.error("âŒ LZ_WEBHOOK_URL ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ .env é…ç½®")

def load_history():
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, "r") as f: return set(json.load(f))
        except: return set()
    return set()

def save_history(history):
    os.makedirs("data", exist_ok=True)
    with open(DB_FILE, "w") as f: json.dump(list(history), f)

async def send_webhook(embed):
    if not WEBHOOK_URL:
        logger.error("æœªé…ç½® LZ_WEBHOOK_URL")
        return
    proxy_url = os.getenv("HTTP_PROXY")
    async with httpx.AsyncClient(proxy=proxy_url) as client:
        for i in range(3):
            try:
                res = await client.post(WEBHOOK_URL, json={"embeds": [embed]}, timeout=15)
                if res.status_code in [200, 204]:
                    return
                # å¦‚æœæ˜¯ 429 é™æµï¼Œå¤šç­‰ä¸€ä¼š
                if res.status_code == 429:
                    await asyncio.sleep(5)
                logger.warning(f"Webhook å‘é€å¤±è´¥ (å°è¯• {i+1}/3): {res.status_code}")
                await asyncio.sleep(2)
            except Exception as e:
                logger.warning(f"Webhook ç½‘ç»œå¼‚å¸¸ (å°è¯• {i+1}/3): {e}")
                await asyncio.sleep(2)
        logger.error("âŒ Webhook å‘é€æœ€ç»ˆå¤±è´¥")

async def push_item(item):
    """æ„å»ºæç®€ç¾åŒ–å¡ç‰‡ (v6.0)"""
    tag = item['tag']
    username = item['username']
    post_id = item['post_id']
    is_video = item['is_video']
    
    # åˆ†ç±»é€»è¾‘å¯¹é½
    # å¦‚æœ tag åœ¨ 5 å¤§å¹³å°ä¸­ï¼Œå®ƒæ˜¯â€œåˆ›ä½œè€…å…¥é©»/åŠ¨æ€â€
    # Load categories dynamically if possible, or use hardcoded list for check
    default_platforms = ["OnlyFans", "Fansly", "Celebrity+Nudes", "Reddit", "Snapchat"]
    is_platform_update = tag in default_platforms or tag not in ['Videos', 'Photos']
    
    title = f"LeakedZone-{tag}åŠ¨æ€"
    color = random.randint(0, 0xFFFFFF)
    
    embed = {
        "title": title,
        "url": item['url'],
        "description": f"å‘ç°æ¥è‡ªåˆ›ä½œè€… **@{username}** çš„æ–°åŠ¨æ€ã€‚\n\n> ğŸ†” å”¯ä¸€æ ‡è¯†: `{post_id}`\n> ğŸ·ï¸ æºæ ‡ç­¾: `{tag}`",
        "color": color,
        "fields": [
            {
                "name": "åˆ›ä½œè€…",
                "value": f"[@{username}]({item['url']})",
                "inline": True
            },
            {
                "name": "åˆ†ç±»",
                "value": "åˆ›ä½œè€…ä¿¡æ¯" if is_platform_update else ("è§†é¢‘" if is_video else "å›¾ç‰‡"),
                "inline": True
            }
        ],
        "footer": {
            "text": f"Power By ä¸œæ–¹éšä¾ å®‰å…¨å›¢é˜Ÿ â€¢ {datetime.now().strftime('%H:%M')}"
        }
    }
    
    # é¢„è§ˆå›¾é€»è¾‘
    # åªæœ‰å½“ç¡®å®æœ‰é¢„è§ˆå›¾ä¸”ä¸æ˜¯ç©ºçš„ï¼Œæ‰è®¾ç½®
    if item['img_url'] and "default" not in item['img_url']:
        embed["image"] = {"url": item['img_url']}
        embed["thumbnail"] = {"url": item['img_url']}
    
    # å¢åŠ ç‚¹å‡»è¯¦æƒ…
    embed["fields"].append({
        "name": "è®¿é—®è¯¦æƒ…",
        "value": f"[ğŸ”— ç‚¹å‡»è®¿é—®è¯¦æƒ…]({item['url']})",
        "inline": False
    })

    if item['created_at']:
        embed["fields"].append({
            "name": "å‘å¸ƒæ—¶é—´",
            "value": f"`{item['created_at']}`",
            "inline": True
        })

    await send_webhook(embed)

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--login", help="Update cookies and UA")
    args = parser.parse_args()

    crawler = LeakedZoneCrawler()
    
    if args.login:
        # ç®€å•åˆ†å‰²ï¼Œæ ¼å¼: "cookie|ua"
        parts = args.login.split('|')
        cookie = parts[0]
        ua = parts[1] if len(parts) > 1 else None
        crawler.set_auth(cookie, ua)
        logger.info("âœ… è®¤è¯å‡­æ®å·²æ›´æ–°")
        if await crawler.check_auth():
            logger.info("âœ¨ å‡­æ®æ ¡éªŒæˆåŠŸï¼")
        else:
            logger.error("âŒ å‡­æ®æ ¡éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥")
        return

    # 1. å¯åŠ¨æ ¡éªŒ
    if not await crawler.check_auth():
        logger.error("ğŸš¨ æ— æ³•é€šè¿‡ LeakedZone éªŒè¯ï¼Œè¯·è¿è¡Œ --login æ›´æ–° Cookie")
        return

    history = load_history()
    all_items = []

    # 2. é‡‡é›†æ•°æ®
    logger.info("âœ…æ­£åœ¨é‡‡é›† (å½“æ—¥è§†é¢‘&å›¾ç‰‡)...")
    all_items.extend(await crawler.crawl_tag("videos"))
    all_items.extend(await crawler.crawl_tag("photos"))

    logger.info("âœ…æ­£åœ¨é‡‡é›†å¹³å°åˆ†ç±»...")
    
    # Load configurable categories
    platforms = ["OnlyFans", "Fansly", "Celebrity+Nudes", "Reddit", "Snapchat"]
    try:
        # ä¼˜å…ˆè¯»å– leakedzone-category.json
        cat_file = "crawlers/leakedzone-category.json"
        if not os.path.exists(cat_file): cat_file = "data/lz_auth.json"
        
        with open(cat_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            if "categories" in data and isinstance(data["categories"], list):
                platforms = data["categories"]
                logger.info(f"å·²åŠ è½½è‡ªå®šä¹‰åˆ†ç±»: {platforms}")
    except: pass

    for p in platforms:
        all_items.extend(await crawler.crawl_category(p))
        await asyncio.sleep(1)

    # 3. å»é‡ä¸æ¨é€
    new_count = 0
    try:
        for item in all_items:
            unique_key = f"{item['tag']}_{item['post_id']}"
            if unique_key not in history:
                logger.info(f"ğŸ†• å‘ç°æ–°åŠ¨æ€: @{item['username']} ({item['tag']})")
                await push_item(item)
                history.add(unique_key)
                new_count += 1
                # å¢é‡ä¿å­˜ï¼Œé˜²æ­¢ä¸­é€”ç”±äºç½‘ç»œå¼‚å¸¸æˆ–é™æµå¯¼è‡´è®°å½•ä¸¢å¤±
                save_history(history)
                await asyncio.sleep(1)
            
            if new_count >= 30: break # å•æ¬¡ä¸Šé™
    except Exception as e:
        logger.error(f"âš ï¸ æ¨é€è¿‡ç¨‹ä¸­æ–­: {e}")
    finally:
        save_history(history)
        logger.info(f"âœ… å¤„ç†å®Œæ¯•ï¼Œå½“å‰å‘¨æœŸæ–°å¢: {new_count}")

if __name__ == "__main__":
    asyncio.run(main())
